[{"content":"Introduction Users running INFINI Console versions 1.29.0 and 1.29.1 might encounter a specific issue after performing a fresh initialization of the platform. If the underlying system Elasticsearch cluster (the one storing Console\u0026rsquo;s metadata, often named .infini_cluster or similar) consists of more than one node, the Console UI might incorrectly report the system cluster\u0026rsquo;s health status as abnormal (e.g., yellow or red).\nThis appears to be a display or status detection artifact specific to these versions under the condition of a newly initialized, multi-node system cluster. The underlying Elasticsearch cluster itself is usually healthy (green).\nThis post provides a straightforward workaround to correct the status displayed in the INFINI Console.\nSymptoms The health status indicator for the \u0026ldquo;System Cluster\u0026rdquo; in the INFINI Console UI shows yellow or red. Accompanying text might indicate an \u0026ldquo;Abnormal\u0026rdquo; or unhealthy status. Checking the actual Elasticsearch system cluster\u0026rsquo;s health directly (e.g., via GET _cluster/health) shows a status: green. This issue is observed only on newly initialized deployments of versions 1.29.0 or 1.29.1 with more than one node in the system Elasticsearch cluster. Solution The fix involves updating a specific document within the .infini_cluster index (or the equivalent index storing your Console\u0026rsquo;s cluster configuration). This document represents the system cluster entity within the Console, and we need to manually set its health status label correctly.\nYou can achieve this by running the following _update_by_query command using Kibana Dev Tools, curl, or any other tool capable of sending requests to your Elasticsearch cluster.\nCommand:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 POST /.infini_cluster/_update_by_query?conflicts=proceed { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;infini_default_system_cluster\u0026#34; } } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.labels = [\u0026#39;health_status\u0026#39;: params.status]\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34; } } } Explanation:\nPOST /.infini_cluster/_update_by_query?conflicts=proceed: Targets the .infini_cluster index (adjust if your system cluster index has a different name) and uses the update-by-query API. conflicts=proceed ensures that if the document is modified between the query and update phases, the operation skips that document instead of failing. query: This finds the specific document representing the default system cluster, identified by id: \u0026quot;infini_default_system_cluster\u0026quot; (verify this ID if you use a custom name). script: This section performs the update using a Painless script. source: \u0026quot;ctx._source.labels = ['health_status': params.status]\u0026quot;: Crucially, this line attempts to set or overwrite a field named labels with a map containing only the key health_status set to the value provided in params.status. lang: \u0026quot;painless\u0026quot;: Specifies the scripting language. params: { \u0026quot;status\u0026quot;: \u0026quot;green\u0026quot; }: Passes the desired status (\u0026ldquo;green\u0026rdquo;) securely into the script. Important Considerations Regarding the Script:\nVerify Field Names: The provided script uses labels and health_status. Please double-check if these are the exact field names INFINI Console 1.29.x uses internally to store the health status label. It\u0026rsquo;s possible the intended fields might be labels and health_status (without the \u0026lsquo;1\u0026rsquo;). If using labels doesn\u0026rsquo;t work, try the alternative script below which targets labels.health_status:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 POST /.infini_cluster/_update_by_query?conflicts=proceed { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;infini_default_system_cluster\u0026#34; } } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.labels == null) { ctx._source.labels = [:]; } ctx._source.labels.health_status = params.status; \u0026#34;\u0026#34;\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34; } } } Overwriting vs. Merging: The original script replaces the entire labels field. If labels might contain other important data, using the alternative script (targeting labels.health_status) is safer as it only adds/updates the specific key.\nRunning the Command:\nKibana: Paste the command into Kibana \u0026gt; Dev Tools and run it. curl: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 curl -X POST \u0026#34;http://YOUR_ELASTICSEARCH_HOST:9200/.infini_cluster/_update_by_query?conflicts=proceed\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;infini_default_system_cluster\u0026#34; } } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.labels = [\u0026#39;health_status\u0026#39;: params.status]\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34; } } }\u0026#39; (Replace YOUR_ELASTICSEARCH_HOST:9200 and potentially the script content based on the verification step above). Verification After Fix After successfully running the _update_by_query command:\nWait a few moments for potential caching to expire. Refresh the INFINI Console web interface. The system cluster status indicator should now correctly display as green (or normal). Conclusion This workaround addresses the specific cosmetic issue of an incorrect system cluster status display in INFINI Console 1.29.0 and 1.29.1 for newly initialized multi-node setups. By manually updating the relevant document using _update_by_query, you can restore the correct status representation in the UI. Remember to verify the exact field names (labels.health_status vs. labels.health_status) if the initial command doesn\u0026rsquo;t yield the expected result. Future versions of INFINI Console are likely to contain a permanent fix for this initialization behavior.\n","date":"2025-04-02T08:00:00Z","image":"https://blog.searchkit.tech/p/fix-infini-console-cluster-status/cover_hu_217718a7cbccac57.png","permalink":"https://blog.searchkit.tech/en/p/fix-infini-console-cluster-status/","title":"Fixing Abnormal System Cluster Status in INFINI Console 1.29.0/1.29.1"},{"content":"Background Our company recently decided to migrate our products to a Kubernetes environment. To better manage and automate our applications, we chose to use Kubernetes Operators. This blog series will document our process of learning and developing Operators, and we hope it can help others get started with Operator development as well.\nTarget Audience Developers and operations personnel with some understanding of Kubernetes. Individuals who want to use Operators to automate application management. People with a basic understanding of the Go language. Prerequisites Before you begin, you\u0026rsquo;ll need the following environment set up:\nGo Language Environment (version 1.23 or higher): Operators are typically developed in Go. You need to install the Go environment, preferably version 1.21 or later (the example uses features compatible with 1.21+, but 1.23+ is recommended per the Kubebuilder requirement often seen). You can download the installation package from https://go.dev/dl/. After installation, please configure your GOPATH and PATH environment variables correctly.\nKubernetes Cluster: You need an accessible Kubernetes cluster to deploy and test the Operator. You can use tools like Minikube, Kind, or any other Kubernetes distribution.\nkubectl Command-Line Tool: kubectl is the Kubernetes command-line tool used to interact with the cluster. Ensure you have kubectl installed, configured, and can connect to your Kubernetes cluster.\nKubebuilder (version 3.0 or higher): Kubebuilder is a framework for rapidly building Kubernetes Operators. Using Kubebuilder simplifies the development process and generates necessary boilerplate code. You can install Kubebuilder using the following commands:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Adjust GOOS/GOARCH if needed, check Kubebuilder docs for specific versions OS=$(go env GOOS) ARCH=$(go env GOARCH) # Verify KubeBuilder version compatibility with your Go and K8s versions # Example for a specific version, check latest releases on Kubebuilder site # curl -L -o kubebuilder https://github.com/kubernetes-sigs/kubebuilder/releases/download/v3.x.y/kubebuilder_${OS}_${ARCH} # Or, get the latest (use with caution, check compatibility) curl -L -o kubebuilder \u0026#34;https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)\u0026#34; # Make it executable and move to a bin directory chmod +x kubebuilder sudo mv kubebuilder /usr/local/bin/ # Or another directory in your PATH, like $HOME/go/bin \u0026gt; Ensure the directory where you place `kubebuilder` (e.g., `/usr/local/bin` or `$HOME/go/bin`) is in your `PATH` environment variable. You can run `kubebuilder version` to verify the installation. Docker (Optional): You need Docker installed if you intend to build Docker images for your Operator. My development environment is macOS (arm64) with OrbStack.\nWhat is an Operator? Simply put, an Operator is an extension to Kubernetes that uses Custom Resources (CRs) to automate the management of applications. Operators allow us to manage complex applications (like databases, message queues, etc.) in the same way we manage built-in Kubernetes resources.\nWhy Choose Operators? Operators provide a declarative way to manage the lifecycle of applications, including deployment, upgrades, backups, and recovery. They can simplify operational workflows, increase automation, and ensure that the application state matches the desired configuration.\nOur First Operator: Hello World This Operator will watch for a Custom Resource named HelloWorld and create a Pod in Kubernetes. This Pod will run a simple application that prints \u0026ldquo;Hello World\u0026rdquo;.\n1. Initialize the Kubebuilder Project First, we need to create a new project using Kubebuilder. Create a new directory within your GOPATH (or any preferred location if using Go modules outside GOPATH), for example, hello-world-operator. Then, navigate into that directory and run the following command:\n1 2 # Replace example.com and repo path with your own details if desired kubebuilder init --domain example.com --repo github.com/your-user/hello-world-operator Note: The original used --domain infini.cloud --repo github.com/infinilabs/hello-world-operator. Remember to replace github.com/your-user/hello-world-operator with your actual repository path if you plan to host it. The domain is used for the API group.\nThis command initializes a new Kubebuilder project, generating several necessary files and directories.\n2. Create the Custom Resource Definition (CRD) Next, we define the structure for our HelloWorld resource. Run the following command:\n1 kubebuilder create api --group example --version v1alpha1 --kind HelloWorld This command creates a new API definition, generating files including api/v1alpha1/helloworld_types.go (for the type definition) and controllers/helloworld_controller.go (for the reconciliation logic).\nEdit the api/v1alpha1/helloworld_types.go file. Modify the HelloWorldSpec definition to include name and message fields:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // HelloWorldSpec defines the desired state of HelloWorld type HelloWorldSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // Name specifies the name for resources created by this HelloWorld resource. // +optional Name string `json:\u0026#34;name,omitempty\u0026#34;` // Message is the message to be printed by the pod. Message string `json:\u0026#34;message\u0026#34;` // Made mandatory for simplicity in example } // HelloWorldStatus defines the observed state of HelloWorld type HelloWorldStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // PodName is the name of the Pod created by the HelloWorld resource. // +optional PodName string `json:\u0026#34;podName,omitempty\u0026#34;` } Note: I\u0026rsquo;ve made Message mandatory (json:\u0026quot;message\u0026quot;) as the controller logic uses it directly. Added +optional markers and a basic Status field as good practice, although the controller logic below doesn\u0026rsquo;t update the status yet. Run make manifests after editing this file.\n3. Implement the Reconcile Logic Edit the controllers/helloworld_controller.go file. Implement the Reconcile function to create a Pod running a busybox image that echoes the message defined in the HelloWorld resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 package controllers import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; apierrors \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/log\u0026#34; examplev1alpha1 \u0026#34;github.com/your-user/hello-world-operator/api/v1alpha1\u0026#34; // !! Update this import path !! ) // HelloWorldReconciler reconciles a HelloWorld object type HelloWorldReconciler struct { client.Client Scheme *runtime.Scheme } //+kubebuilder:rbac:groups=example.com,resources=helloworlds,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=example.com,resources=helloworlds/status,verbs=get;update;patch //+kubebuilder:rbac:groups=example.com,resources=helloworlds/finalizers,verbs=update //+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete // Reconcile is part of the main kubernetes reconciliation loop which aims to // move the current state of the cluster closer to the desired state. func (r *HelloWorldReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { log := log.FromContext(ctx) log.Info(\u0026#34;Reconciling HelloWorld\u0026#34;) // 1. Fetch the HelloWorld instance helloWorld := \u0026amp;examplev1alpha1.HelloWorld{} if err := r.Get(ctx, req.NamespacedName, helloWorld); err != nil { if apierrors.IsNotFound(err) { // Object not found, likely deleted. Return without error. log.Info(\u0026#34;HelloWorld resource not found. Ignoring since object must be deleted\u0026#34;) return ctrl.Result{}, nil } // Error reading the object - requeue the request. log.Error(err, \u0026#34;Failed to get HelloWorld\u0026#34;) return ctrl.Result{}, err } // Use spec.Name if provided, otherwise default to CR name for the Pod name prefix podNamePrefix := helloWorld.Spec.Name if podNamePrefix == \u0026#34;\u0026#34; { podNamePrefix = helloWorld.Name } podName := podNamePrefix + \u0026#34;-pod\u0026#34; // 2. Define the desired Pod desiredPod := \u0026amp;corev1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: podName, Namespace: helloWorld.Namespace, Labels: map[string]string{ \u0026#34;app\u0026#34;: helloWorld.Name, // Label with the CR name for easier selection }, }, Spec: corev1.PodSpec{ Containers: []corev1.Container{ { Name: \u0026#34;hello-world-container\u0026#34;, // More descriptive container name Image: \u0026#34;busybox\u0026#34;, // Use echo and sleep; echo prints the message from the spec Command: []string{\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, fmt.Sprintf(\u0026#34;echo \u0026#39;Message from %s: %s\u0026#39;; sleep 3600\u0026#34;, helloWorld.Name, helloWorld.Spec.Message)}, }, }, RestartPolicy: corev1.RestartPolicyOnFailure, // Example policy }, } // 3. Set HelloWorld instance as the owner and controller of the Pod // This ensures the Pod is garbage collected when the HelloWorld CR is deleted if err := ctrl.SetControllerReference(helloWorld, desiredPod, r.Scheme); err != nil { log.Error(err, \u0026#34;Failed to set controller reference for Pod\u0026#34;) return ctrl.Result{}, err } // 4. Check if the Pod already exists foundPod := \u0026amp;corev1.Pod{} err := r.Get(ctx, client.ObjectKey{Name: desiredPod.Name, Namespace: desiredPod.Namespace}, foundPod) // If Pod does not exist, create it if err != nil \u0026amp;\u0026amp; apierrors.IsNotFound(err) { log.Info(\u0026#34;Creating a new Pod\u0026#34;, \u0026#34;Pod.Namespace\u0026#34;, desiredPod.Namespace, \u0026#34;Pod.Name\u0026#34;, desiredPod.Name) err = r.Create(ctx, desiredPod) if err != nil { log.Error(err, \u0026#34;Failed to create new Pod\u0026#34;, \u0026#34;Pod.Namespace\u0026#34;, desiredPod.Namespace, \u0026#34;Pod.Name\u0026#34;, desiredPod.Name) return ctrl.Result{}, err } // Pod created successfully - don\u0026#39;t requeue immediately, wait for watch event log.Info(\u0026#34;Pod created successfully\u0026#34;) return ctrl.Result{}, nil // Usually better to rely on watches than requeueing immediately } else if err != nil { // Other error trying to get the Pod log.Error(err, \u0026#34;Failed to get Pod\u0026#34;) return ctrl.Result{}, err } // 5. Pod already exists - potentially check/update if needed (skipped for this simple example) log.Info(\u0026#34;Skip reconcile: Pod already exists\u0026#34;, \u0026#34;Pod.Namespace\u0026#34;, foundPod.Namespace, \u0026#34;Pod.Name\u0026#34;, foundPod.Name) // If we needed to update the pod based on CR changes, we would do it here. // For this example, we do nothing if the Pod exists. // (Optional) Update HelloWorld status - Good practice but omitted for brevity here // helloWorld.Status.PodName = foundPod.Name // if err := r.Status().Update(ctx, helloWorld); err != nil { // log.Error(err, \u0026#34;Failed to update HelloWorld status\u0026#34;) // return ctrl.Result{}, err // } return ctrl.Result{}, nil } // SetupWithManager sets up the controller with the Manager. func (r *HelloWorldReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;examplev1alpha1.HelloWorld{}). // Watch for HelloWorld resources Owns(\u0026amp;corev1.Pod{}). // Watch for Pods owned by HelloWorld Complete(r) } Important: Replace github.com/your-user/hello-world-operator in the import path with the actual path you used in kubebuilder init --repo. Run make manifests generate after editing the controller file to update generated code and manifests.\n4. Install the CRD into the Kubernetes Cluster Run the following command to install the Custom Resource Definition into your cluster:\n1 make install 5. Run the Operator Run the following command to run the Operator locally (it will connect to your configured Kubernetes cluster):\n1 make run Keep this terminal running.\n6. Create the HelloWorld Resource In a new terminal, create a file named my-hello-world.yaml with the following content:\n1 2 3 4 5 6 7 8 apiVersion: example.com/v1alpha1 # Ensure group matches your --group flag kind: HelloWorld metadata: name: my-hello-world-sample # Changed name slightly to avoid conflict with potential Pod name logic namespace: default # Specify namespace or use kubectl default spec: name: my-hello # Name used for the Pod prefix message: \u0026#34;Hello World from my first Operator!\u0026#34; Apply this resource using kubectl:\n1 kubectl apply -f my-hello-world.yaml 7. Verify Check if the Pod was created (using the spec.name field + -pod suffix as defined in the controller):\n1 kubectl get pods -n default You should see a Pod named my-hello-pod (or similar based on your spec.name).\nCheck the logs of the Pod to confirm it printed the message:\n1 2 # Replace my-hello-pod with the actual pod name from \u0026#39;kubectl get pods\u0026#39; kubectl logs my-hello-pod -n default You should see output similar to: Message from my-hello-world-sample: Hello World from my first Operator!\nConclusion Congratulations on creating your first Operator! While this example is simple, it demonstrates the fundamental principle of Operators: watching Custom Resources and managing Kubernetes resources based on their desired state. In the upcoming parts of this series, we will delve into more advanced Operator features.\nStay tuned for the next post!\n","date":"2025-04-01T08:00:00Z","image":"https://blog.searchkit.tech/p/operator/cover_hu_85673dec5e4f59e6.png","permalink":"https://blog.searchkit.tech/en/p/operator/","title":"Getting Started with Operator Development Series (Part 1): Hello World!"},{"content":" The Internet was done so well that most people think of it as a natural resource like the Pacific Ocean, rather than something that was man-made. When was the last time a tech‐ nology with a scale like that was so error-free?\n— Alan Kay, in interview with Dr Dobb’s Journal (2012)\nMany applications today are data-intensive, as opposed to compute-intensive. Raw CPU power is rarely a limiting factor for these applications—bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing.\nA data-intensive application is typically built from standard building blocks that pro‐ vide commonly needed functionality. For example, many applications need to:\nStore data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream pro‐ cessing) Periodically crunch a large amount of accumulated data (batch processing) If that sounds painfully obvious, that’s just because these data systems are such a suc‐ cessful abstraction: we use them all the time without thinking too much. When build‐ ing an application, most engineers wouldn’t dream of writing a new data storage engine from scratch, because databases are a perfectly good tool for the job.\nBut reality is not that simple. There are many database systems with different charac‐ teristics, because different applications have different requirements. There are vari‐ ous approaches to caching, several ways of building search indexes, and so on. When building an application, we still need to figure out which tools and which approaches are the most appropriate for the task at hand. And it can be hard to combine tools when you need to do something that a single tool cannot do alone.\nThis book is a journey through both the principles and the practicalities of data sys‐ tems, and how you can use them to build data-intensive applications. We will explore what different tools have in common, what distinguishes them, and how they achieve their characteristics.\nIn this chapter, we will start by exploring the fundamentals of what we are trying to achieve: reliable, scalable, and maintainable data systems. We’ll clarify what those things mean, outline some ways of thinking about them, and go over the basics that we will need for later chapters. In the following chapters we will continue layer by layer, looking at different design decisions that need to be considered when working on a data-intensive application.\n…… Summary In this chapter, we have explored some fundamental ways of thinking about data-intensive applications. These principles will guide us through the rest of the book, where we dive into deep technical detail.\nAn application has to meet various requirements in order to be useful. There are functional requirements (what it should do, such as allowing data to be stored, retrieved, searched, and processed in various ways), and some nonfunctional require‐ ments (general properties like security, reliability, compliance, scalability, compatibil‐ ity, and maintainability). In this chapter we discussed reliability, scalability, and maintainability in detail.\nReliability means making systems work correctly, even when faults occur. Faults can be in hardware (typically random and uncorrelated), software (bugs are typically sys‐ tematic and hard to deal with), and humans (who inevitably make mistakes from time to time). Fault-tolerance techniques can hide certain types of faults from the end user.\nScalability means having strategies for keeping performance good, even when load increases. In order to discuss scalability, we first need ways of describing load and performance quantitatively. We briefly looked at Twitter’s home timelines as an example of describing load, and response time percentiles as a way of measuring performance. In a scalable system, you can add processing capacity in order to remain reliable under high load.\nMaintainability has many facets, but in essence it’s about making life better for the engineering and operations teams who need to work with the system. Good abstrac‐ tions can help reduce complexity and make the system easier to modify and adapt for new use cases. Good operability means having good visibility into the system’s health, and having effective ways of managing it.\nThere is unfortunately no easy fix for making applications reliable, scalable, or main‐ tainable. However, there are certain patterns and techniques that keep reappearing in different kinds of applications. In the next few chapters we will take a look at some examples of data systems and analyze how they work toward those goals.\nLater in the book, in Part III, we will look at patterns for systems that consist of sev‐ eral components working together, such as the one in Figure 1-1.\nReferences Michael Stonebraker and Uğur Çetintemel: “\u0026lsquo;One Size Fits All\u0026rsquo;: An Idea Whose Time Has Come and Gone,” at 21st International Conference on Data Engineering (ICDE), April 2005.\nWalter L. Heimerdinger and Charles B. Weinstock: “A Conceptual Framework for System Fault Tolerance,” Technical Report CMU/SEI-92-TR-033, Software Engineering Institute, Carnegie Mellon University, October 1992.\nDing Yuan, Yu Luo, Xin Zhuang, et al.: “Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems,” at 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2014.\nYury Izrailevsky and Ariel Tseitlin: “The Netflix Simian Army,” techblog.netflix.com, July 19, 2011.\nDaniel Ford, François Labelle, Florentina I. Popovici, et al.: “Availability in Globally Distributed Storage Systems,” at 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2010.\nBrian Beach: “Hard Drive Reliability Update – Sep 2014,” backblaze.com, September 23, 2014.\nLaurie Voss: “AWS: The Good, the Bad and the Ugly,” blog.awe.sm, December 18, 2012.\nHaryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: “What Bugs Live in the Cloud?,” at 5th ACM Symposium on Cloud Computing (SoCC), November 2014. doi:10.1145/2670979.2670986\nNelson Minar: “Leap Second Crashes Half the Internet,” somebits.com, July 3, 2012.\nAmazon Web Services: “Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region,” aws.amazon.com, April 29, 2011.\nRichard I. Cook: “How Complex Systems Fail,” Cognitive Technologies Laboratory, April 2000.\nJay Kreps: “Getting Real About Distributed System Reliability,” blog.empathybox.com, March 19, 2012.\nDavid Oppenheimer, Archana Ganapathi, and David A. Patterson: “Why Do Internet Services Fail, and What Can Be Done About It?,” at 4th USENIX Symposium on Internet Technologies and Systems (USITS), March 2003.\nNathan Marz: “Principles of Software Engineering, Part 1,” nathanmarz.com, April 2, 2013.\nMichael Jurewitz:“The Human Impact of Bugs,” jury.me, March 15, 2013.\nRaffi Krikorian: “Timelines at Scale,” at QCon San Francisco, November 2012.\nMartin Fowler: Patterns of Enterprise Application Architecture. Addison Wesley, 2002. ISBN: 978-0-321-12742-6\nKelly Sommers: “After all that run around, what caused 500ms disk latency even when we replaced physical server?” twitter.com, November 13, 2014.\nGiuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: “Dynamo: Amazon\u0026rsquo;s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Systems Principles (SOSP), October 2007.\nGreg Linden: “Make Data Useful,” slides from presentation at Stanford University Data Mining class (CS345), December 2006.\nTammy Everts: “The Real Cost of Slow Time vs Downtime,” webperformancetoday.com, November 12, 2014.\nJake Brutlag:“Speed Matters for Google Web Search,” googleresearch.blogspot.co.uk, June 22, 2009.\nTyler Treat: “Everything You Know About Latency Is Wrong,” bravenewgeek.com, December 12, 2015.\nJeffrey Dean and Luiz André Barroso: “The Tail at Scale,” Communications of the ACM, volume 56, number 2, pages 74–80, February 2013. doi:10.1145/2408776.2408794\nGraham Cormode, Vladislav Shkapenyuk, Divesh Srivastava, and Bojian Xu: “Forward Decay: A Practical Time Decay Model for Streaming Systems,” at 25th IEEE International Conference on Data Engineering (ICDE), March 2009.\nTed Dunning and Otmar Ertl: “Computing Extremely Accurate Quantiles Using t-Digests,” github.com, March 2014.\nGil Tene: “HdrHistogram,” hdrhistogram.org.\nBaron Schwartz: “Why Percentiles Don’t Work the Way You Think,” vividcortex.com, December 7, 2015.\nJames Hamilton: “On Designing and Deploying Internet-Scale Services,” at 21st Large Installation System Administration Conference (LISA), November 2007.\nBrian Foote and Joseph Yoder: “Big Ball of Mud,” at 4th Conference on Pattern Languages of Programs (PLoP), September 1997.\nFrederick P Brooks: “No Silver Bullet – Essence and Accident in Software Engineering,” in The Mythical Man-Month, Anniversary edition, Addison-Wesley, 1995. ISBN: 978-0-201-83595-3\nBen Moseley and Peter Marks: “Out of the Tar Pit,” at BCS Software Practice Advancement (SPA), 2006.\nRich Hickey: “Simple Made Easy,” at Strange Loop, September 2011.\nHongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson: “Analyzing Software Evolvability,” at 32nd Annual IEEE International Computer Software and Applications Conference (COMPSAC), July 2008. doi:10.1109/COMPSAC.2008.50\n","date":"2025-03-06T00:00:00Z","image":"https://blog.searchkit.tech/p/ddia/cover_hu_7063f67e7c1f8890.png","permalink":"https://blog.searchkit.tech/en/p/ddia/","title":"Reliable, Scalable, and Maintainable Applications"}]